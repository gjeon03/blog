---
title: "Speech to Text with Web Speech API"
publishedAt: "2025-06-16"
summary: "Web Speech APIì˜ SpeechRecognition ì¸í„°í˜ì´ìŠ¤ë¥¼ í™œìš©í•´ ë¸Œë¼ìš°ì €ì—ì„œ STT(Speech-to-Text) ê¸°ëŠ¥ì„ êµ¬í˜„í•œ ì˜ˆì œì™€ í•µì‹¬ ê°œë…ì„ ì •ë¦¬í•©ë‹ˆë‹¤."
tags: ["speech-recognition", "web-speech-api", "react", "typescript", "stt"]
---

# Web Speech APIë¡œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸° (STT)

ì´ì „ì— ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ í•˜ë©´ì„œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¾¸ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•œ ì ì´ ìˆëŠ”ë° ë¸Œë¼ìš°ì €ë§Œìœ¼ë¡œ ì´ëŸ° ê¸°ëŠ¥ì„ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ”ê²Œ ì¬ë¯¸ìˆì—ˆê³  ë‹¹ì‹œ ê¸‰í•˜ê²Œ ê¸°ëŠ¥êµ¬í˜„ë§Œ í•˜ëŠë¼ ë‹¤ì‹œ í•œë²ˆ í•™ìŠµì„ í•˜ê³  ì‹¶ì—ˆìŠµë‹ˆë‹¤.

## Web Speech API - `SpeechRecognition` ì¸í„°í˜ì´ìŠ¤

`SpeechRecognition`ì€ Web Speech API ì¤‘ í•˜ë‚˜ë¡œ, ì‚¬ìš©ìì˜ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” **STT(Speech-to-Text)** ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì´ ì¸í„°í˜ì´ìŠ¤ëŠ” ë¸Œë¼ìš°ì €ì˜ ìŒì„± ì¸ì‹ ì—”ì§„ì„ ì œì–´í•˜ëŠ” **ì»¨íŠ¸ë¡¤ëŸ¬ ì—­í• **ì„ í•˜ë©°, ì¸ì‹ ê²°ê³¼ë¥¼ ë‹´ì€ `SpeechRecognitionEvent` ê°ì²´ë„ í•¨ê»˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.

> ğŸ“˜ [MDN ì„¤ëª…](https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition):
>
> â€œThe `SpeechRecognition` interface of the Web Speech API is the controller interface for the recognition service; this also handles the `SpeechRecognitionEvent` sent from the recognition service.â€

### âš ï¸ ë¸Œë¼ìš°ì € í˜¸í™˜ì„± ì£¼ì˜

ëª¨ë“  ë¸Œë¼ìš°ì €ì—ì„œ `SpeechRecognition`ì„ ì§€ì›í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.

ğŸ‘‰ [SpeechRecognition ë¸Œë¼ìš°ì € í˜¸í™˜ì„± í™•ì¸ (MDN)](https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition#browser_compatibility)

## TypeScriptì—ì„œ ì‚¬ìš©í•  ë•Œ ì£¼ì˜í•  ì 

Web Speech APIëŠ” ì•„ì§ ê³µì‹ì ì¸ TypeScript íƒ€ì… ì •ì˜ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ê³¼ê±°ì—ëŠ” `@types/webspeechapi` íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì—¬ ì‚¬ìš©í–ˆë˜ê²ƒ ê°™ì€ë° í˜„ì¬ëŠ” deprecated(ë” ì´ìƒ ìœ ì§€ë³´ìˆ˜ë˜ì§€ ì•ŠìŒ) ìƒíƒœë¡œ ê·¸ëƒ¥ ì‚¬ìš©í•˜ê±°ë‚˜ ë”°ë¡œ ì§ì ‘ íƒ€ì…ì„ ì„ ì–¸í•´ì„œ ì‚¬ìš©í•´ì•¼ë©ë‹ˆë‹¤.

Web Speech API ê°€ ì•„ì§ ì¼ë¶€ ë¸Œë¼ìš°ì €ì—ì„œë§Œ ì œí•œì ìœ¼ë¡œ ì§€ì›ë˜ê³  ìˆê¸° ë•Œë¬¸ì´ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤.

```tsx
npm install -D @types/webspeechapi
```

ì €ëŠ” `@types/webspeechapi` íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ í–ˆìŠµë‹ˆë‹¤.

# êµ¬í˜„í•˜ê¸°

## 1. VoiceRecorder

```tsx
//voice-recorder.tsx
import {
  SpeechRecognitionLanguage,
  useSpeechRecognition,
} from "@/hooks/use-speech-recognition";

const languageOptions: { code: SpeechRecognitionLanguage; label: string }[] = [
  { code: "ko-KR", label: "í•œêµ­ì–´" },
  { code: "en-US", label: "English (US)" },
  { code: "ja-JP", label: "æ—¥æœ¬èª" },
  { code: "zh-CN", label: "ä¸­æ–‡ (ç®€ä½“)" },
  { code: "fr-FR", label: "FranÃ§ais" },
  { code: "es-ES", label: "EspaÃ±ol" },
];

export default function VoiceRecorder() {
  const [lang, setLang] = useState<SpeechRecognitionLanguage>("ko-KR");
  const {
    transcript,
    isListening,
    startListening,
    stopListening,
    resetTranscript,
  } = useSpeechRecognition({
    lang,
    onError: (event) => {
      console.log("ìŒì„± ì¸ì‹ ì—ëŸ¬ ë°œìƒ:", event.error);
    },
  });

  const handleToggleRecording = () => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  };

  const handleResetTranscript = () => {
    resetTranscript();
  };

  return (
    <div className="flex flex-col items-center justify-center h-screen gap-4 px-4 text-center">
      <h1 className="text-2xl font-bold">Speech to Text</h1>

      {/* ì–¸ì–´ ì„ íƒ */}
      <select
        className="select border"
        value={lang}
        onChange={(e) => setLang(e.target.value as SpeechRecognitionLanguage)}
        disabled={isListening}
      >
        {languageOptions.map(({ code, label }) => (
          <option key={code} value={code}>
            {label}
          </option>
        ))}
      </select>

      {/* ìŒì„± ì¸ì‹ ìƒíƒœ í‘œì‹œ */}
      <button
        className={`px-4 py-2 text-white rounded cursor-pointer w-48 ${
          isListening ? "bg-red-500" : "bg-blue-500"
        }`}
        onClick={handleToggleRecording}
      >
        {isListening ? "ìŒì„± ì¸ì‹ ì¤‘ì§€" : "ìŒì„± ì¸ì‹ ì‹œì‘"}
      </button>

      {/* STT ìƒíƒœ í‘œì‹œ */}
      <span className="text-sm text-gray-500">
        {isListening ? "ğŸ™ï¸ ìŒì„± ì¸ì‹ ì¤‘..." : "ğŸ›‘ ìŒì„± ì¸ì‹ ëŒ€ê¸° ì¤‘"}
      </span>

      {/* ë…¹ìŒ í…ìŠ¤íŠ¸ */}
      <div className="min-h-[4rem] mt-2 text-gray-700 whitespace-pre-line">
        {transcript || "ğŸ¤ ìŒì„±ì„ ì…ë ¥í•´ë³´ì„¸ìš”..."}
      </div>

      {/* ë¦¬ì…‹ ë²„íŠ¼ */}
      <button
        className="mt-2 px-3 py-1 text-sm text-gray-700 bg-gray-200 rounded hover:bg-gray-300 cursor-pointer"
        onClick={handleResetTranscript}
        disabled={!transcript}
      >
        ğŸ§¹ í…ìŠ¤íŠ¸ ì§€ìš°ê¸°
      </button>
    </div>
  );
}
```

## 2. useSpeechRecognition.ts

```tsx
import { useRef, useState } from "react";

export type SpeechRecognitionLanguage =
  // ì•„ì‹œì•„
  | "ko-KR" // í•œêµ­ì–´ (ëŒ€í•œë¯¼êµ­)
  | "ja-JP" // ì¼ë³¸ì–´ (ì¼ë³¸)
  | "zh-CN" // ì¤‘êµ­ì–´ (ì¤‘êµ­ - ê°„ì²´)
  | "zh-TW" // ì¤‘êµ­ì–´ (ëŒ€ë§Œ - ë²ˆì²´)
  | "zh-HK" // ì¤‘êµ­ì–´ (í™ì½© - ë²ˆì²´)
  | "th-TH" // íƒœêµ­ì–´ (íƒœêµ­)
  | "hi-IN" // íŒë””ì–´ (ì¸ë„)
  | "id-ID" // ì¸ë„ë„¤ì‹œì•„ì–´ (ì¸ë„ë„¤ì‹œì•„)
  | "ms-MY" // ë§ë ˆì´ì–´ (ë§ë ˆì´ì‹œì•„)

  // ìœ ëŸ½
  | "en-GB" // ì˜ì–´ (ì˜êµ­)
  | "en-US" // ì˜ì–´ (ë¯¸êµ­)
  | "en-AU" // ì˜ì–´ (í˜¸ì£¼)
  | "en-CA" // ì˜ì–´ (ìºë‚˜ë‹¤)
  | "en-IN" // ì˜ì–´ (ì¸ë„)
  | "de-DE" // ë…ì¼ì–´ (ë…ì¼)
  | "fr-FR" // í”„ë‘ìŠ¤ì–´ (í”„ë‘ìŠ¤)
  | "fr-CA" // í”„ë‘ìŠ¤ì–´ (ìºë‚˜ë‹¤)
  | "it-IT" // ì´íƒˆë¦¬ì•„ì–´ (ì´íƒˆë¦¬ì•„)
  | "es-ES" // ìŠ¤í˜ì¸ì–´ (ìŠ¤í˜ì¸)
  | "es-MX" // ìŠ¤í˜ì¸ì–´ (ë©•ì‹œì½”)
  | "pt-BR" // í¬ë¥´íˆ¬ê°ˆì–´ (ë¸Œë¼ì§ˆ)
  | "pt-PT" // í¬ë¥´íˆ¬ê°ˆì–´ (í¬ë¥´íˆ¬ê°ˆ)
  | "nl-NL" // ë„¤ëœë€ë“œì–´ (ë„¤ëœë€ë“œ)
  | "ru-RU" // ëŸ¬ì‹œì•„ì–´ (ëŸ¬ì‹œì•„)
  | "pl-PL" // í´ë€ë“œì–´ (í´ë€ë“œ)
  | "sv-SE" // ìŠ¤ì›¨ë´ì–´ (ìŠ¤ì›¨ë´)
  | "da-DK" // ë´ë§ˆí¬ì–´ (ë´ë§ˆí¬)
  | "fi-FI" // í•€ë€ë“œì–´ (í•€ë€ë“œ)
  | "no-NO" // ë…¸ë¥´ì›¨ì´ì–´ (ë…¸ë¥´ì›¨ì´)

  // ì•„í”„ë¦¬ì¹´ ë° ì¤‘ë™
  | "ar-SA" // ì•„ëì–´ (ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„)
  | "ar-EG" // ì•„ëì–´ (ì´ì§‘íŠ¸)
  | "tr-TR" // í„°í‚¤ì–´ (í„°í‚¤)
  | "fa-IR" // í˜ë¥´ì‹œì•„ì–´ (ì´ë€)
  | "he-IL" // íˆë¸Œë¦¬ì–´ (ì´ìŠ¤ë¼ì—˜)
  | "af-ZA"; // ì•„í”„ë¦¬ì¹¸ìŠ¤ì–´ (ë‚¨ì•„í”„ë¦¬ì¹´ê³µí™”êµ­)

interface UseSpeechRecognitionOptions {
  lang?: SpeechRecognitionLanguage;
  onError?: (error: SpeechRecognitionError) => void;
}

export const useSpeechRecognition = ({
  lang = "ko-KR",
  onError,
}: UseSpeechRecognitionOptions = {}) => {
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState("");

  const startListening = () => {
    if (
      !("webkitSpeechRecognition" in window || "SpeechRecognition" in window)
    ) {
      console.error("Speech recognition not supported");
      return;
    }

    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = lang;

    recognition.onstart = () => {
      setIsListening(true);
    };

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      let finalTranscript = "";
      for (let i = event.resultIndex; i < event.results.length; ++i) {
        const result = event.results[i];
        if (result.isFinal) {
          finalTranscript += result[0].transcript;
        }
      }

      if (finalTranscript) {
        setTranscript((prev) => prev + finalTranscript);
        console.log("Final result:", finalTranscript);
      }
    };

    recognition.onerror = (event: SpeechRecognitionError) => {
      console.error("Speech recognition error", event.error);
      setIsListening(false);
      onError?.(event);
    };

    recognition.onend = () => {
      setIsListening(false);
    };

    recognition.start();
    recognitionRef.current = recognition;
  };

  const stopListening = () => {
    recognitionRef.current?.stop();
    setIsListening(false);
  };

  const resetTranscript = () => {
    setTranscript("");
  };

  return {
    isListening,
    transcript,
    startListening,
    stopListening,
    resetTranscript,
  };
};
```

# í•µì‹¬ í¬ì¸íŠ¸ ì •ë¦¬

### 1. continuous

- `true`ë¡œ ì„¤ì •í•˜ë©´ ëŠê¸°ì§€ ì•Šê³  ê³„ì† ìŒì„±ì„ ì¸ì‹í•©ë‹ˆë‹¤.
- ê¸°ë³¸ê°’ì€ `false`ì´ë©°, ì´ ê²½ìš° ì‚¬ìš©ìê°€ ì ì‹œ ë§ì„ ë©ˆì¶”ë©´ ìë™ìœ¼ë¡œ ì¸ì‹ì´ ì¢…ë£Œë©ë‹ˆë‹¤.

### 2. interimResults

- `true`ì¼ ê²½ìš° ì¤‘ê°„ ì¸ì‹ ê²°ê³¼(í™•ì •ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸)ë„ ë°›ì•„ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì¤‘ê°„ ê²°ê³¼ëŠ” `isFinal: false` ìƒíƒœë¡œ `event.results[i]`ì— ë‹´ê²¨ ì˜µë‹ˆë‹¤.

### 3. lang

- ì¸ì‹í•  ì–¸ì–´ (ì˜ˆ: `"ko-KR"`, `"en-US"`)

### 4. onstart, onend

- ì¸ì‹ ì‹œì‘/ì¢…ë£Œ ì‹œì  ì½œë°±

### 5. onresult

- ìŒì„± ì¸ì‹ ê²°ê³¼ ì½œë°±
- `event.results`ëŠ” ì´ì¤‘ ë°°ì—´ë¡œ êµ¬ì„±ë˜ë©°, ê° ê²°ê³¼ì—ëŠ” `isFinal` ì†ì„±ìœ¼ë¡œ ìµœì¢… í™•ì • ì—¬ë¶€ê°€ í‘œì‹œ

### 6. onerror

- ì¸ì‹ ì‹¤íŒ¨ ì‹œ ì½œë°±

### 7. start(), stop()

- ì¸ì‹ ì‹œì‘/ì •ì§€ ë©”ì„œë“œ

## íŠ¹ì´ì‚¬í•­

í•˜ë‚˜ íŠ¹ì´í–ˆë˜ê±´ ìŒì„± ì¸ì‹ì´ ì¼ì •ì‹œê°„ ì•ˆë˜ê³  ìˆìœ¼ë©´ â€œno-speechâ€ ë¼ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ì™€ í•¨ê»˜ ìŒì„± ì¸ì‹ì´ ì¢…ë£Œëœë‹¤ëŠ” ê±°ì˜€ìŠµë‹ˆë‹¤. ì´ë¡œì¸í•´ ìŒì„± ì¸ì‹ê³¼ ë™ì‹œì— ë…¹ìŒì„ í•œë‹¤ê±°ë‚˜ ìŒëŸ‰ ì‹œê°í™”ë¥¼ í‘œì‹œí•  ê²½ìš°ì—” ì´ ì¢…ë£Œ ì‹œì ì„ í•¨ê»˜ ê³ ë ¤í•´ì•¼ ë  ê±° ê°™ì•˜ìŠµë‹ˆë‹¤.

# ì‘ë™ ì˜ˆì œ

<iframe
  src="https://codesandbox.io/embed/w6v524?view=preview&module=%2Fsrc%2FApp.tsx&hidenavigation=1"
  style={{
    width: "100%",
    height: "500px",
    border: "1px #000 solid",
    borderRadius: "4px",
    overflow: "hidden",
  }}
  title="speech-to-text"
  allow="accelerometer; ambient-light-sensor; camera; encrypted-media; geolocation; gyroscope; hid; microphone; midi; payment; usb; vr; xr-spatial-tracking"
  sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
/>

# Reference

- [MDN SpeechRecognition](https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition)
